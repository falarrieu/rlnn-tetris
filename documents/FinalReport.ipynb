{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLNN Tetris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Federico Larrieu and Tyson O'Leary, May 5, 2023*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We both really enjoy Tetris and we are interested in applying reinforcement learning to a more complex problem than we've seen in class. Applying it to tetris will be constantly interesting, and the research we've done shows it also will not be trivial. \n",
    "\n",
    "For our project, we will create an AI to play Tetris. It will use a neural network and reinforcement learning to learn to play tetris optimally. This will require us to either find an implementation of Tetris or implement the game ourselves, then define our states and actions to be used by the Q-net. \n",
    "\n",
    "The questions we will seek to answer:\n",
    "  * What is the highest number of cleared lines our AI can reach?\n",
    "  * What is the highest score our AI can achieve?\n",
    "  * Can we split this problem into multiple neural networks to solve different parts of the problem?\n",
    "\n",
    "We hypothesize that copying the reinforcement learning methods we learned in class most likely will not produce satisfactory results. We plan to explore multiple definitions for the actions and states, as well as multiple architectures like an ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What, why, very brief overview of methods and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach to creating an AI for Tetris involves breaking down the problem into subproblems. The first subproblem is determining the best placement for a piece, while the second subproblem involves moving the piece into the desired position and rotation. If time permits, we may also incorporate a piece selection strategy. We plan to use two reinforced neural networks, each responsible for one subproblem, and deploy them in an ensemble model architecture.\n",
    "\n",
    "First, the model will input a still frame of the game. If a piece is being held, both the held piece and the current piece will be run through the position neural network, and the position with the better placement will be output. If no piece is being held, then the next piece and the current piece will be run through the position neural network, and again the position with the better placement will be output. The best position will then be fed into the movement neural network, which will determine the next best action to take to move towards that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"EnsembleModel.drawio.svg\" alt= “EnsembleModel” width=\"500\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Reinforced Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PositionRLNN.drawio.svg\" alt= “PositionRLNN” width=\"400\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PositionTraining.drawio.svg\" alt= “PositionTraining” width=\"400\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movement Reinforced Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MovementRLNN.drawio.svg\" alt= “MovementRLNN” width=\"400\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MovementTraining.drawio.svg\" alt= “MovementTraining” width=\"400\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movement Neural Network Hyperparameter Optimization\n",
    "\n",
    "\n",
    "Switching to a DQN has introduced new hyperparameters that we have not learned or used in class. This includes batch size and tau. We approached hyperparameter optimization by first individually optimizing each hyperparameter whilst maintaing the other hyperparameters constant. This approach was to understand the each new hyperparameter would affect accuracy of a DQN model. We had the same approach with the hyperparameters that we had learned before. This was to verify that using a DQN model will not act differently then what we have seen before.\n",
    "\n",
    "Using what we gained from individually optimizing and understanding the trends of each hyperparameter, we ran a simple grid search without cross validation. This was done simply with the use of several nested for loops in which each would iterate through a list of different values for the respective hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch\n",
    "\n",
    "\n",
    "   Batch size helps determine the number of transistions that will be sampled from the replay memory in order to optimize the model. The larger the batch size the more stable our model will perform. Smaller batch sizes introduce more noise into the model thus leading into instability. Transitions which are held in the replay memory buffer are composed of the current state, action, next state, and a tuple of rewards. Each of the composed components are then transposed into batch tensors. With these tensor we can then compute the Q values of the state and actions within our transitions. We then compute the loss, minimize the loss using the chosen optimizer (Adam or SGD), and update the parameters of the neural network.\n",
    "\n",
    "![OptBatch.png](OptBatch.png)\n",
    "<p style=\"text-align: center;\">Figure 6</p>\n",
    "\n",
    "\n",
    "   To optimize for the batch size, we researched common ranges of batch sizes and decided to test using batch sizes of 32, 64, and 128. For each batch size, we kept the other hyperparameters constant, including GAMMA = 0.25, EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 1000, TAU = 0.005, LR = 1e-4, n_hiddens_per_layer=[512, 256, 128], num_episodes = 500, and optimizer=\"adam\". In our results (Fig 6.) we observe that a batch size of 64 achieved a final average test accuracy of %35 and and batch size of 128 achieved a final average test accuracy of %37. Being that the accuracy of 64 and 128 batch sizes are quite similar, we might want to choose a batch size 64 to save on computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma\n",
    "\n",
    "Gamma is the discount factor that is used in the Bellman equation. Gamma is used to weigh the importance of future rewards relevant to immediate rewards. The closer the value is to 0 means we are wanting to weigh immediate rewards larger. If the value is closer to 1 than we are placing more emphasis on future rewards.\n",
    "\n",
    "![OptGamma.png](OptGamma.png)\n",
    "<p style=\"text-align: center;\">Figure 7</p>\n",
    "\n",
    "   To optimize for the gamma, we tested using gamma values of 0.1, 0.25, 0.5, 0.75 and 0.9. For each value, we kept the other hyperparameters constant, including BATCH_SIZE= 128, EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 1000, TAU = 0.005, LR = 1e-4, n_hiddens_per_layer=[512, 256, 128], num_episodes = 500, and optimizer=\"adam\". In our result we observed the best final accuracy of %35 with a gamma value of 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tau\n",
    "\n",
    "\n",
    "Tau determines the rate at which the target network's weights are updated with the weights of the policy network. In other words, the target network's weights are updated by interpolating with the policy network's weights with a rate of Tau. The purpose of doing this is to provide a more stable set of targets for the Q-value function. The higher the Tau value, the slower the update rate of the target network, and vice versa.\n",
    "\n",
    "![OptTau.png](OptTau.png)\n",
    "<p style=\"text-align: center;\">Figure 8</p>\n",
    "\n",
    "To optimize for the tau, we tested using values of 0.001, 0.005, 0.01, 0.05 and 0.1. For each value, we kept the other hyperparameters constant, including BATCH_SIZE= 128, GAMMA=0.25, EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 1000, LR = 1e-4, n_hiddens_per_layer=[512, 256, 128], num_episodes = 500, and optimizer=\"adam\". In our result we observed the best accuracy %37 with a tau value of 0.01.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "\n",
    "The learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function during training of a neural network.\n",
    "\n",
    "![OptLearningRate.png](OptLearningRate.png)\n",
    "<p style=\"text-align: center;\">Figure 9</p>\n",
    "\n",
    "To optimize for the learning rate, we tested using values of 0.0001, 0.001, 0.01 and 0.1. For each value, we kept the other hyperparameters constant, including BATCH_SIZE= 128, GAMMA=0.25, EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 1000, TAU = 0.005, n_hiddens_per_layer=[512, 256, 128], num_episodes = 500, and optimizer=\"adam\". In our result we observed the best accuracy %38 with a learning rate value of 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N Hiddens Per Layer\n",
    "\n",
    "The number of hidden units per layer is a hyperparameter that determines the architecture of the neural network. A larger number of hidden units can potentially improve the model's ability to capture patterns in the data, but it can also increase overfitting.\n",
    "\n",
    "![OptArch.png](OptArch.png)\n",
    "<p style=\"text-align: center;\">Figure 10</p>\n",
    "\n",
    "To optimize for the number of hidden layers, we tested using different architectures of [50, 25], [512, 256, 128], [1024, 512, 256, 128]. For each architecture, we kept the other hyperparameters constant, including BATCH_SIZE= 128, GAMMA=0.25, EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 1000, TAU = 0.005,  LR = 1e-4, num_episodes = 500, and optimizer=\"adam\". In our result we observed the best accuracy was achieved from the largest network architecture. Although, the second best network architecture was close in accuracy and may be a better choice to save on computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodes\n",
    "\n",
    "Episodes are in essence the number of times we allow the agent to interact with the tetris enviroment. Specifically in context of the movement agent, an episode was moving a piece from the to finish or failure. \n",
    "\n",
    "![OptEpisodes.png](OptEpisodes.png)\n",
    "<p style=\"text-align: center;\">Figure #</p>\n",
    "\n",
    "To optimize for the number of episodes, we tested using the values 10, 1000, 5000 and 10000. For each value, we kept the other hyperparameters constant, including BATCH_SIZE= 128, GAMMA=0.25, EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 1000, TAU = 0.005, LR = 1e-4, n_hiddens_per_layer=[512, 256, 128], and optimizer=\"adam\". In our result we observed the the 1000, 5000 and 10000 episodes seem to be quite close in accuracy. In our final model we experiment with all three values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "\n",
    "\n",
    "![GridSearchDF.png](GridSearchDF.png)\n",
    "<p style=\"text-align: center;\">Figure #</p>\n",
    "\n",
    "\n",
    "- optimized hyperparamters\n",
    "- number of episodes\n",
    "- test accuracy\n",
    "- (maybe show animation of model performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In working on this project, we gained valuable knowledge and further solidified our understanding of machine learning techniques. During the inception phase, we quickly reviewed various implementations of the Tetris problem, which allowed us to observe the flexibility of problem-solving in machine learning. Each paper had similar but different approaches to tackling the Tetris problem, giving us the ability to be more creative in composing our solution.\n",
    "\n",
    "Our hypothesis was that decomposing the Tetris problem into subproblems would enable us to assign responsibility to different agents, each finely tuned to handle one problem. However, when implementing the agent responsible for moving the tetronimos (MovementNN), we initially encountered issues with the agent not learning patterns, even though there were no obvious errors in implementation. We suspected the large game state space of Tetris might be causing the problem, but after testing the agent through 100,000s of trials, we found no improvement.\n",
    "\n",
    "With the due date fast approaching, we decided to proceed with using the Deep Q Learning methodology. We re-implemented the neural network, incorporated replay memory, and added a seperate trainer implementation. Our efforts yielded positive results with near 50% accuracy in testing, without hyperparameter optimization and under 1000 episodes. After, we began to conduct hyperparameter optimization as well as increading the number of episodes. **Results**\n",
    "\n",
    "Moving forward, we would like to ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I learned.  What was difficult.  Changes I had to make to timeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions\n",
    "\n",
    "**Tyson O'Leary**\n",
    "- Tetronimos\n",
    "- Piece Provider \n",
    "- Valid Placement Generator\n",
    "- Board\n",
    "- Game\n",
    "- MovementNN\n",
    "- Neural Network \n",
    "- Neural Network 2\n",
    "- NNTrainer\n",
    "- Vector\n",
    "- Presentation\n",
    "- Report\n",
    "\n",
    "**Federico Larrieu**\n",
    "- Client\n",
    "- Board\n",
    "- Game\n",
    "- MovementNN\n",
    "- Neural Network\n",
    "- Neural Network 2\n",
    "- NNTrainer\n",
    "- Hyperparameter Optimization\n",
    "- Presentation\n",
    "- Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Goodfellow, et al., 2016] Ian Goodfellow and Yoshua Bengio and Aaron Courville, [Deep Learning](http://www.deeplearningbook.org), MIT Press. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Your report for a single person team should contain **approximately** 2,000 words times number of team members, in markdown cells.  You can count words by running the following python code in your report directory.  Projects with two people, for example, should contain about 4,000 words, a four-person team should submit a report of approximately 8,000 words.\n",
    "\n",
    "Of course, your results and analysis speak much more than the word count.  Deep analysis in a shorter form is better than vague, over-wordy non-analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'warnings' has no attribute 'warning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnbformat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nbfile \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProject Report Example.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/latest/lib/python3.9/site-packages/nbformat/current.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mwarnings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarning\u001b[49m(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124;03m\"\"\"nbformat.current is deprecated.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m- use nbformat for read/write/validate public API\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m- use nbformat.vX directly to composing notebooks of a particular version\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraitlets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnbformat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v3 \u001b[38;5;28;01mas\u001b[39;00m _v_latest\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'warnings' has no attribute 'warning'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from nbformat import current\n",
    "import glob\n",
    "nbfile = glob.glob('Project Report Example.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
