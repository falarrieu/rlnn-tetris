{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal for CS445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tyson O'Leary and Freddy Larrieu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project, we will create an AI to play Tetris. It will use a neural network and reinforcement learning to learn to play tetris optimally. This will require us to either find an implementation of Tetris or implement the game ourselves, then define our states and actions to be used by the Q-net. \n",
    "\n",
    "The questions we will seek to answer:\n",
    "  * What is the highest number of cleared lines our AI can reach?\n",
    "  * What is the highest score our AI can achieve?\n",
    "  * Can we split this problem into multiple neural networks to solve different parts of the problem?\n",
    "\n",
    "We hypothesize that copying the reinforcement learning methods we learned in class most likely will not produce satisfactory results. We plan to explore multiple definitions for the actions and states, as well as multiple architectures like an ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We both really enjoy Tetris and we are interested in applying reinforcement learning to a more complex problem than we've seen in class. Applying it to tetris will be constantly interesting, and the research we've done shows it also will not be trivial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build off of the neural networks we have created in class to make our reinforced learning networks. We also already have an implementation of Tetris that we can use to test and present our networks, and possibly to train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To approach creating an AI for Tetris we have decided to decompose Tetris into its different problems. The first of which is to decide the best placement for a piece. The second is how to move the piece into a specified position and rotation. We may also introduce new features, if time permits, such as piece selection strategy. Splitting the Tetris strategy into two different individual problems will allow to allocate the responsibility to two different reinforced neural networks. We will deploy both reinforced neural networks into an ensemble model architecture. The flow of the ensemble model architecture is as follows. First, a still frame of the game will be input, if a piece is held we will run both the piece held and the current piece through the position neural network. The position of the piece with the better placement will be output. If no piece is held, then the next piece and the current piece through the position neural network. The position of the piece with the better placement will be output. Next, the best position will be used in the input for the movement neural network, which will decide the next best action to move towards that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EnsembleModel](https://raw.githubusercontent.com/falarrieu/rlnn-tetris/ff01e9f8818749a87ba39b1addeebd77de699b82/documents/EnsembleModel.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postion Reinforced Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that learns to find the best possible end position of a piece will take in all pixels of the board and the type of piece as input, and output the best landing position and rotation. Each training epoch, the network will try a possible position, and get a reward based on a few factors. This might include the height of the piece, whether it creates holes, and the number of lines it would clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PositionRLNN](https://raw.githubusercontent.com/falarrieu/rlnn-tetris/ff01e9f8818749a87ba39b1addeebd77de699b82/documents/PositionRLNN.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PositionTraining](https://raw.githubusercontent.com/falarrieu/rlnn-tetris/ff01e9f8818749a87ba39b1addeebd77de699b82/documents/PositionTraining.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movement Reinforced Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that learns how to move a piece from a start position to an end position has many more inputs. It also takes all of the board pixels, but also the start position and rotation, the end position and rotation, and all possible movements. It will output the correct next movement to move toward the end position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MovementRLNN](https://raw.githubusercontent.com/falarrieu/rlnn-tetris/ff01e9f8818749a87ba39b1addeebd77de699b82/documents/MovementRLNN.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MovementTraining](https://raw.githubusercontent.com/falarrieu/rlnn-tetris/ff01e9f8818749a87ba39b1addeebd77de699b82/documents/MovementTraining.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Results\n",
    "\n",
    "#### *What is the highest number of cleared lines our AI can reach?*\n",
    "\n",
    "The world record for highest number of lines cleared in Tetris (by a human) is 4,988 lines by Harry Hong. So, 5000 cleared lines would be a good starting goal for our AI. However, others have created Tetris AIs before and achieved much higher results, so it is very possible we could do the same.\n",
    "\n",
    "#### *What is the highest score our AI can achieve?*\n",
    "\n",
    "This will be a secondary goal to the number of lines cleared. More lines cleared at once means more points, but it may depend on whether we incentivize survival or score more, because often clearing single lines over and over could be an easier way to survive. However, it should still be possible to get a high score. Our goal will be 999,999.\n",
    "\n",
    "#### *Can we split this problem into multiple neural networks to solve different parts of the problem?*\n",
    "\n",
    "We hypothesize that we can split the problem into two reinforcement learning neural networks. One would learn how to move a piece from a starting position to a landing position and the other would learn to choose the best end position given the current piece and board state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "Make a list with at least four entries with dates and describe what each team member will accomplish by these dates.  This is for your use.  Your grade will not depend on meeting these deadlines.\n",
    "\n",
    "1. Complete input data generation - by 4/10\n",
    "    - Board and Noise - Freddy\n",
    "    - Possible Moves - Tyson\n",
    "2. Complete Python implementation and training of best move nerual network - by 4/20 \n",
    "    - Train Function - Tyson\n",
    "    - Neural Network - Freddy\n",
    "3. Complete Python implementation and training of movement neural network - by 4/30\n",
    "    - Train Function - Freddy\n",
    "    - Neural Network - Tyson\n",
    "4. Complete Python implementation of Tetris - by 5/3 Freddy & Tyson\n",
    "4. Combine the networks to play Tetris - by 5/11 Freddy & Tyson\n",
    "\n",
    "Stretch:\n",
    "\n",
    "5. Implement Tetris in frontend React JS app and access the neural network through API endpoints.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
